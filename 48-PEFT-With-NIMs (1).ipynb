{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "932e3a28-f5b5-497d-be11-3f29417a01a2",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0127a4-11ad-4da8-98ad-169eb1fdd79e",
   "metadata": {},
   "source": [
    "# Using NIMs for PEFT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712cef2f-fdc5-4858-814c-494dbcb9c73b",
   "metadata": {},
   "source": [
    "This workshop was first delivered in March of 2024, and since that time, although it was really not so long ago, some very significant changes have occured in the LLM ecosystem, relevant to the topic of Parameter Efficient Fine-Tuning. We would like to take the time in this notebook to briefly update you on some of the changes that we believe are most relevant, and provide you with some resources that will help you take everything you've learned in the workshop today and bridge into some new and improved ways of writing LLM-based applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2390d9-8022-49b9-ab6c-bc0358ed7776",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4ff0c8-3ffe-4ab2-8455-5ec32f0d2632",
   "metadata": {},
   "source": [
    "## NVIDIA NIMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc80f39-968d-43d2-9734-e49287f9abec",
   "metadata": {},
   "source": [
    "Perhaps the most important is NVIDIA's release of NIMs. NIMs are containerized \"microservices\" that can be used for a variety of LLM-based applications. Most relevant to our work are language model NIMs, which in summary, support the incredibly easy deployment of performance optimized LLMs on a variety of infrastrctures. At the time of writing this (August 2024) some of the LLMs available as NIMs are Llama 3.1, and Mistral/Mixtral models. You can play around with sandbox of available NIMs at [build.nvidia.com](https://build.nvidia.com/explore/discover).\n",
    "\n",
    "To put it simply, NIMs have made it super easy to deploy LLMs that are far superior to the models we used in this course, and we highly recommend you look into using them in your own applications. Typically, developers will try out NIMs via [the model playground](https://build.nvidia.com/explore/discover) we mentioned above, do a little bit more prototyping using an API key to integrate the NIMs on build.nvidia.com into an application, and then when ready, download and use local NIMs for full-fledged application development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3d3c6-e597-402d-9de9-8f03a9436b89",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9309e19-3216-4242-8a61-fb64033cda58",
   "metadata": {},
   "source": [
    "## NIMs and LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e76b69-b52b-4f53-99d1-9ab04a3eba39",
   "metadata": {},
   "source": [
    "Furthermore, NIMs support LoRA out of the box, and assuming you have LoRA adapters as a result of performing PEFT (see more details on this topic below), it's incredibly simple to use NIMs to serve one or many LoRA fine-tuned models along side the base LLM, much as you have experienced in using Nemo Service today.\n",
    "\n",
    "You can see examples of this super simple workflow in the [PEFT section of the NIM documentation](https://docs.nvidia.com/nim/large-language-models/latest/peft.html) and in this NVIDIA tech blog called [_A Simple Guide to Deploying Generative AI with NVIDIA NIM_](https://developer.nvidia.com/blog/a-simple-guide-to-deploying-generative-ai-with-nvidia-nim/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e0d189-ec96-41fd-909c-e5903d6d883f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642acefc-f7c7-4837-a9c3-c6c6187a9e2f",
   "metadata": {},
   "source": [
    "## Obtaining LoRA Adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c236b40e-2937-4df3-ab01-62437d43693f",
   "metadata": {},
   "source": [
    "We have used Nemo Service to perform the actual LoRA fine-tuning today. Nemo Service also took care of managing the resulting LoRA adapters on our behalf.\n",
    "\n",
    "If you are working with NIMs, you will need to use a different methodology to perform LoRA fine-tuning and obtain the LoRA adapters which you can then deploy very easily with your NIMs as shown in the previous section of this notebook.\n",
    "\n",
    "In fact there are a myriad of ways to go about doing this, but we have 2 in particular which we recommend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24dfa0a2-3abe-42fb-8f9c-a3edd5009fa5",
   "metadata": {},
   "source": [
    "### On HuggingFace via AutoTrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0105122-675a-459e-ba16-f63e1e554b40",
   "metadata": {},
   "source": [
    "When working with NIMs you are working largely with open source models which you can also find on HuggingFace. Conveniently, you can visit these models' HuggingFace pages where you will find an option to train (or fine-tune) the models using DGX Cloud, which are NVIDIA-backed computational resources.\n",
    "\n",
    "Using this feature, you can perform LoRA via a high-level web interface. In summary, you provide your training and validation data, specify some hyperparameters and kick of the training. The workflow is very similar to how you performed PEFT in this course.\n",
    "\n",
    "See the technical blog post [_Easily Train Models with H100 GPUs on NVIDIA DGX Cloud_](https://huggingface.co/blog/train-dgx-cloud) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8a6fa6-9d65-4e8c-9028-fdc580f5b41b",
   "metadata": {},
   "source": [
    "### With Nemo Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db9fa7-fdd7-4175-902f-794332554c2e",
   "metadata": {},
   "source": [
    "We've already mentioned this technique earlier in the course, but assuming you have a local version of a base model you want to perform LoRA fine-tuning on, Nemo Framework gives you a rather straight-forward way to perform the LoRA fine-tuning on your own infrastructure.\n",
    "\n",
    "For more detailed instructions, follow along with [this documentation](https://docs.nvidia.com/nemo-framework/user-guide/latest/llms/gemma/peft.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
