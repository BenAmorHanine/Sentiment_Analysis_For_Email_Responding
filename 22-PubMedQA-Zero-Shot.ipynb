{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13467d12-0e4e-442d-b67f-73c0f89e8038",
   "metadata": {},
   "source": [
    "![NVIDIA Logo](images/nvidia.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030b50c7-e8a4-4ee1-838d-554e8d0281dc",
   "metadata": {},
   "source": [
    "# PubMedQA With Zero-Shot Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b4e284-875a-4234-ba26-bb1098ca8ff7",
   "metadata": {},
   "source": [
    "In this notebook we'll obtain baseline performances for several of our available LLMs by performing and evaluating zero shot prompting on the PubMedQA question answering task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d43cf-6631-4062-813d-0a9e69a5442b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62a71f-32c5-4ee6-a8de-50f6dba53a1e",
   "metadata": {},
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b39abe4-5145-427d-b2ac-69e4ef429186",
   "metadata": {},
   "source": [
    "By the time you complete this notebook you will:\n",
    "- Format the raw PubMedQA data to be more suitable for prompting.\n",
    "- Evaluate zero-shot performance for 3 GPT models on PubMedQA Data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237c056d-e8df-4ca1-a5db-6dbc07f018aa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a87aeb-b2bd-4f2e-bcbd-5b6dbc607737",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfd69037-5e1b-48be-baf5-69505f45e280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from llm_utils.nemo_service_models import NemoServiceBaseModel\n",
    "from llm_utils.models import PubmedModels\n",
    "from llm_utils.helpers import plot_experiment_results, accuracy_score\n",
    "from llm_utils.pubmedqa import strip_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac39d57c-8f34-48d0-8077-727a63517253",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e45fb3e-d39b-49ad-8ea8-ea60d636defe",
   "metadata": {},
   "source": [
    "## List Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e64b8be-3e19-4e28-a63e-7c92f025f4ae",
   "metadata": {},
   "source": [
    "While working with the PubMedQA data, we will be using several of the GPT models provided to us by NeMo Service, which we've collected for you in the `PubmedModels` enum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ee9b1a-0a92-4310-a086-1d855323fb2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt8b: gpt-8b-000\n",
      "gpt20b: gpt20b\n",
      "gpt43b: gpt-43b-001\n"
     ]
    }
   ],
   "source": [
    "PubmedModels.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1036de-f290-42a1-9cf2-a745c75035fc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe054cd-9674-4589-945e-b2e14b669eb4",
   "metadata": {},
   "source": [
    "## PubMedQA for Customization Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69a937c-3464-42aa-98ea-f4ad818d1ee3",
   "metadata": {},
   "source": [
    "The PubMedQA question answering task will require our models to both perform reasoning using a lot of very specialized terminology, and also format responses in a very specific way. Additionally it is clear and well-labeled dataset. For these reasons it will serve us well as an entrypoint into PEFT, providing us not only the data needed for PEFT fine-tuning, but also as an opportunity to observe performance across a variety of models and customization techniques including zero and few-shot prompting, which will help us make a thorough quantitative analysis of how PEFT can benefit our use of these LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24def7d7-6ef9-4f9a-8c47-a2c8fb42c6b5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03328482-b0fc-429e-acb8-d8570774fe8a",
   "metadata": {},
   "source": [
    "## Load Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe2890c-b884-412a-abbd-26bb9fc53a13",
   "metadata": {},
   "source": [
    "Here we load the test split of the PubMedQA data created in the last notebook, which will give us a dictionary of 150 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "990f64ef-009c-4f1a-a1cc-e9c15e9e6128",
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmedqa_data = json.load(open('data/pubmedqa_test.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93cd6e1c-f1f0-4cbe-a144-9ba646b56c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pubmedqa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aaf8f76-0833-4426-9103-a975b8212d90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pubmedqa_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf2ca75-717e-4571-a952-ac984eda6553",
   "metadata": {},
   "source": [
    "Here we print the first entry in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4502b631-1580-4747-8778-e9088310131f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'QUESTION': \"The English antibiotic awareness campaigns: did they change the public's knowledge of and attitudes to antibiotic use?\",\n",
       " 'CONTEXTS': ['To determine the effect of the 2008 English public antibiotic campaigns.',\n",
       "  'English and Scottish (acting as controls) adults aged>or = 15 years were questioned face to face about their attitudes to and use of antibiotics, in January 2008 (1888) before and in January 2009 (1830) after the antibiotic campaigns.',\n",
       "  'Among English respondents, there was a small increase in recollection of campaign posters (2009 23.7% versus 2008 19.2%; P = 0.03), but this increase was only 2.3% higher in England than in Scotland. We did not detect any improvement in either England or Scotland, or any differences between England and Scotland in the understanding of the lack of benefit of antibiotics for coughs and colds, and we found no improvement in antibiotic use. We detected a significant increase in respondents retaining leftover antibiotics. Over 20% reported discussing antibiotics with their general practitioner (GP) or nurse in the year to January 2009. The offer of a delayed antibiotic prescription was reported significantly more often by English respondents (19% versus 8% Scottish in 2009; P = 0.01), and English respondents were advised to use other remedies for coughs and colds significantly more often in the year to January 2009 (12.7% in 2009 versus 7.4% in 2008; P<0.001).'],\n",
       " 'LABELS': ['OBJECTIVES', 'METHODS', 'RESULTS'],\n",
       " 'MESHES': ['Adolescent',\n",
       "  'Adult',\n",
       "  'Aged',\n",
       "  'Aged, 80 and over',\n",
       "  'Anti-Bacterial Agents',\n",
       "  'Bacterial Infections',\n",
       "  'Data Collection',\n",
       "  'England',\n",
       "  'Female',\n",
       "  'Health Knowledge, Attitudes, Practice',\n",
       "  'Health Services Research',\n",
       "  'Humans',\n",
       "  'Male',\n",
       "  'Middle Aged',\n",
       "  'Scotland',\n",
       "  'Young Adult'],\n",
       " 'YEAR': '2010',\n",
       " 'reasoning_required_pred': 'yes',\n",
       " 'reasoning_free_pred': 'no',\n",
       " 'final_decision': 'no',\n",
       " 'LONG_ANSWER': 'There is little evidence that the 2008 public antibiotic campaigns were effective. The use and visibility of future campaign materials needs auditing. A carefully planned approach that targets the public in GP waiting rooms and through clinicians in consultations may be a more effective way of improving prudent antibiotic use.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(pubmedqa_data.values())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142b0337-6dc6-481b-ad0a-72d5e51f9946",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d9bc86-c2a3-4ae0-b1c1-85dd9763216e",
   "metadata": {},
   "source": [
    "## Process Data in Prep for Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6d1c4d-cd16-42bb-906b-385d0dfd78ea",
   "metadata": {},
   "source": [
    "Given the printout we just saw, let's reformat the data to be in a more suitable format for prompting. To do this we'll use `generate_prompts_with_answers` which for a given PubMedQA data entry will return a 2-tuple containing a prompt-formatted version of the entry, and its corresponding label.\n",
    "\n",
    "We've created this prompt format for you but please keep in mind as you go out to work with your own data that any time you need to convert data into a prompt for an LLM, time and care should be taken with the prompt engineering process to arrive at prompts that work well for your situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f9479d1-f0f2-4eab-a833-03875cace5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompts_with_answers(data):\n",
    "    prompt = \"\"\n",
    "    for index, context in enumerate(data['CONTEXTS']):\n",
    "        section_label = data['LABELS'][index]\n",
    "        prompt += f\"{section_label}: {context}\\n\"\n",
    "    \n",
    "    question_text = data['QUESTION']\n",
    "    prompt += f\"QUESTION: {question_text}\\n\"\n",
    "    prompt += f\"ANSWER (yes|no|maybe): \"\n",
    "\n",
    "    label = data['final_decision']\n",
    "    \n",
    "    return (prompt, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c056e04-1f96-4f4e-9565-a292876309c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_and_answers = []\n",
    "for value in pubmedqa_data.values():\n",
    "    prompts_and_answers.append(generate_prompts_with_answers(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c731972-73bc-41fd-897c-8ab3a1017edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts_and_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92ceed9f-50b8-4058-8aaa-0ad74e4d615d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_prompt = prompts_and_answers[0][0]\n",
    "sample_answer = prompts_and_answers[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f434d6a-213f-4d39-8124-1d0b5e6970d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OBJECTIVES: To determine the effect of the 2008 English public antibiotic campaigns.\n",
      "METHODS: English and Scottish (acting as controls) adults aged>or = 15 years were questioned face to face about their attitudes to and use of antibiotics, in January 2008 (1888) before and in January 2009 (1830) after the antibiotic campaigns.\n",
      "RESULTS: Among English respondents, there was a small increase in recollection of campaign posters (2009 23.7% versus 2008 19.2%; P = 0.03), but this increase was only 2.3% higher in England than in Scotland. We did not detect any improvement in either England or Scotland, or any differences between England and Scotland in the understanding of the lack of benefit of antibiotics for coughs and colds, and we found no improvement in antibiotic use. We detected a significant increase in respondents retaining leftover antibiotics. Over 20% reported discussing antibiotics with their general practitioner (GP) or nurse in the year to January 2009. The offer of a delayed antibiotic prescription was reported significantly more often by English respondents (19% versus 8% Scottish in 2009; P = 0.01), and English respondents were advised to use other remedies for coughs and colds significantly more often in the year to January 2009 (12.7% in 2009 versus 7.4% in 2008; P<0.001).\n",
      "QUESTION: The English antibiotic awareness campaigns: did they change the public's knowledge of and attitudes to antibiotic use?\n",
      "ANSWER (yes|no|maybe): \n"
     ]
    }
   ],
   "source": [
    "print(sample_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d767f1c9-afa2-406b-befa-e9f7b3145f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b187a2-4670-4827-b54b-c0c1062fed26",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d45d6-11ed-4349-9f78-61c1f995a6d4",
   "metadata": {},
   "source": [
    "## Write Formatted Prompts and Answers to File"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2fc9a2-fc2f-453d-9a4e-4ef8f8f0e50d",
   "metadata": {},
   "source": [
    "We will be reusing `prompts_and_answers` in the next several notebooks so here we write it to file for easy re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9a6c6d6-d67e-42f6-a761-b3e71568e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/pubmedqa_panda_test.json', 'w') as f:\n",
    "    json.dump(prompts_and_answers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48479854-ba70-41f3-a344-d69c31205535",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b930ca5-0725-48df-8841-422fb09566c5",
   "metadata": {},
   "source": [
    "## Try Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eaa1e44-a759-463e-9a05-de3f53446151",
   "metadata": {},
   "source": [
    "We will begin our PubMedQA experiments with zero shot attempts using a GPT43B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "515a67be-5197-48ec-a3b6-8289fad103b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt43b = NemoServiceBaseModel(PubmedModels.gpt43b.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b739c35-ee9e-47ec-baa8-10b0ab31ec80",
   "metadata": {},
   "source": [
    "Because answers are expected to be `yes`, `no`, or `maybe` we limit token generation to 1 token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a5ca594-cd42-451d-8407-b07816422506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from model:  no\n",
      "Actual answer: no\n",
      "Response from model correct: False\n",
      "\n",
      "Response from model:  no\n",
      "Actual answer: no\n",
      "Response from model correct: False\n",
      "\n",
      "Response from model:  yes\n",
      "Actual answer: yes\n",
      "Response from model correct: False\n",
      "\n",
      "Response from model:  no\n",
      "Actual answer: no\n",
      "Response from model correct: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt, answer in prompts_and_answers[::45]:\n",
    "    response = gpt43b.generate(prompt, tokens_to_generate=1)\n",
    "    print(f'Response from model: {response}')\n",
    "    print(f'Actual answer: {answer}')\n",
    "    correct = response == answer\n",
    "    print(f'Response from model correct: {correct}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2ab4e1-5197-4e4d-b450-240b2fe82f8f",
   "metadata": {},
   "source": [
    "We can see that the model did well with this small sample of answers in terms of generating the correct answer, however, it did insert some extra white space.\n",
    "\n",
    "Part of what we want to evaluate a fine-tuned model on is its ability to generate well-formatted responses, so with that in mind we do not, for the sake of these experiments, want to do a lot of post-processing. However, let's agree that stripping white space is almost always okay and do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc723fd6-3a12-4824-b390-c8e5f1be885c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from model: no\n",
      "Actual answer: no\n",
      "Response from model correct: True\n",
      "\n",
      "Response from model: no\n",
      "Actual answer: no\n",
      "Response from model correct: True\n",
      "\n",
      "Response from model: yes\n",
      "Actual answer: yes\n",
      "Response from model correct: True\n",
      "\n",
      "Response from model: no\n",
      "Actual answer: no\n",
      "Response from model correct: True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for prompt, answer in prompts_and_answers[::45]:\n",
    "    response = gpt43b.generate(prompt, tokens_to_generate=1).strip() # Strip white space\n",
    "    print(f'Response from model: {response}')\n",
    "    print(f'Actual answer: {answer}')\n",
    "    correct = response == answer\n",
    "    print(f'Response from model correct: {correct}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189e5697-0543-4579-8e4b-be90a11ea352",
   "metadata": {},
   "source": [
    "At least with this small sample size, GPT43B appears to do very well with the PubMedQA task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a1ebac-3522-4d6b-a136-26037face529",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580b22e-27dc-45da-881f-2aaffd94a402",
   "metadata": {},
   "source": [
    "## Experiment with Several GPT Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6d8dd-12d2-40bf-bb5f-b9873a961d65",
   "metadata": {},
   "source": [
    "To get a better sample size of performance for our experiements, let's go ahead and utilize 3 different-sized GPT models. Here we create a dictionary `llms` containing all 3 of the GPT model instance types loaded into this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad036a36-5e11-4902-86e5-eb3280a028e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llms = {model.name: NemoServiceBaseModel(model.value) for model in PubmedModels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43b37638-6927-47be-b9d6-2c674ab9d0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt8b': <llm_utils.nemo_service_models.NemoServiceBaseModel at 0x7f1e7078a270>,\n",
       " 'gpt20b': <llm_utils.nemo_service_models.NemoServiceBaseModel at 0x7f1e70788a10>,\n",
       " 'gpt43b': <llm_utils.nemo_service_models.NemoServiceBaseModel at 0x7f1e70788200>}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff9a755-d6c0-4c24-aa45-1a4b168cf267",
   "metadata": {},
   "source": [
    "Now we will perform a sanity check for all of them on the first 3 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "878bb913-5226-4137-a56c-f9d604ccf22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT8B\n",
      "-----\n",
      "\n",
      "Response from model: 1\n",
      "Actual answer: no\n",
      "Response from model correct: False\n",
      "\n",
      "Response from model: 1\n",
      "Actual answer: yes\n",
      "Response from model correct: False\n",
      "\n",
      "Response from model: 1\n",
      "Actual answer: yes\n",
      "Response from model correct: False\n",
      "\n",
      "GPT20B\n",
      "------\n",
      "\n"
     ]
    },
    {
     "ename": "ServerSideError",
     "evalue": "Request failed with HTTP Status Code 504 GATEWAY TIMEOUT **Solution**: Server is unable to handle your request right now. Please retry your request after a brief wait. If this problem persist, please contact NeMo LLM team **Full response**: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center></center>\r\n</body>\r\n</html>\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerSideError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00munderline\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt, answer \u001b[38;5;129;01min\u001b[39;00m prompts_and_answers[:\u001b[38;5;241m3\u001b[39m]:\n\u001b[0;32m----> 6\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_to_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mResponse from model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/workspace/dli/2-PubMedQA/llm_utils/nemo_service_models.py:67\u001b[0m, in \u001b[0;36mNemoServiceBaseModel.generate\u001b[0;34m(self, prompt, return_type, tokens_to_generate, temperature, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_stream(prompt, model, customization_id, tokens_to_generate, temperature, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustomization_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_to_generate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/dli/2-PubMedQA/llm_utils/nemo_service_models.py:97\u001b[0m, in \u001b[0;36mNemoServiceBaseModel._generate_text\u001b[0;34m(self, prompt, model, customization_id, tokens_to_generate, temperature, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03mInternal method to generate text in text mode.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m conn \u001b[38;5;241m=\u001b[39m NemoLLM(api_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_host, api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key)\n\u001b[0;32m---> 97\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustomization_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustomization_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens_to_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens_to_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/nemollm/api.py:460\u001b[0m, in \u001b[0;36mNemoLLM.generate\u001b[0;34m(self, model, prompt, customization_id, return_type, tokens_to_generate, logprobs, temperature, top_p, top_k, stop, random_seed, repetition_penalty, beam_search_diversity_rate, beam_width, length_penalty, disable_logging)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 460\u001b[0m     \u001b[43mNemoLLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NemoLLM\u001b[38;5;241m.\u001b[39mpost_process_generate_response(response, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/nemollm/api.py:130\u001b[0m, in \u001b[0;36mNemoLLM.handle_response\u001b[0;34m(response, stream)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ClientSideError(status_code\u001b[38;5;241m=\u001b[39mstatus_code, reason\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mreason, decoded_content\u001b[38;5;241m=\u001b[39mdecoded_content)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# server side errors\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServerSideError(status_code\u001b[38;5;241m=\u001b[39mstatus_code, reason\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mreason, decoded_content\u001b[38;5;241m=\u001b[39mdecoded_content)\n",
      "\u001b[0;31mServerSideError\u001b[0m: Request failed with HTTP Status Code 504 GATEWAY TIMEOUT **Solution**: Server is unable to handle your request right now. Please retry your request after a brief wait. If this problem persist, please contact NeMo LLM team **Full response**: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center></center>\r\n</body>\r\n</html>\r\n"
     ]
    }
   ],
   "source": [
    "for name, llm in llms.items():\n",
    "    underline = \"-\"*len(name)\n",
    "    print(f'{name.upper()}\\n{underline}\\n')\n",
    "    \n",
    "    for prompt, answer in prompts_and_answers[:3]:\n",
    "        response = llm.generate(prompt, tokens_to_generate=1).strip()\n",
    "        print(f'Response from model: {response}')\n",
    "        print(f'Actual answer: {answer}')\n",
    "        correct = response == answer\n",
    "        print(f'Response from model correct: {correct}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0abe48-c581-490e-991d-702d427b3031",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2864dd0-a95d-4433-bf2d-e27c820ee013",
   "metadata": {},
   "source": [
    "So far the 8B model gave us complete junk, the 20B model gave us somewhat well formatted but not always correct responses, and the 43B model appears to be doing great."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bb12bd-b490-47a2-a0a8-c8746238f6fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa29244-313a-4722-8213-3888dea7c8b3",
   "metadata": {},
   "source": [
    "## The NemoServiceBaseModel Evaluate Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed7a68-7724-459f-8d1b-849512d29146",
   "metadata": {},
   "source": [
    "Our `NemoServiceBaseModel` has an `evaluate` method we will be using. In its most simple form we can simply pass it a list of 2-tuples containing prompt/label pairs (like `prompts_and_answers`) and it will generate a response for every prompt, check the response against the label, and at the end of its run print an accuracy score for the run.\n",
    "\n",
    "Since evaluate is calling `generate` under the hood, we can also pass in other generation parameters like `tokens_to_generate`, `top_k`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff335d81-626a-4c28-8ee7-97cd7027e69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "776af2f77ec14b3daa57490752ba7f88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3 correct\n",
      "Accuracy: 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.00'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt43b.evaluate(prompts_and_answers[:3], tokens_to_generate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5971a5-2091-4f71-8a60-db01e5c7e991",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b1ff94-21b9-4909-8d5a-e510cd732ecf",
   "metadata": {},
   "source": [
    "## Post Processing With Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf8cac-2fba-40cc-8948-1aa55848a825",
   "metadata": {},
   "source": [
    "By default `evaluate` will not perform any post processing on the model output. With what we observed above, it makes sense that `gpt43b` scored 0 on the first 2 samples since we know it is including some white space at the beginning of the response.\n",
    "\n",
    "If we'd like we can pass a `get_clean_prediction` function to `evaluate` which should expect a model response string, perform some post-processing on it, and return the results which are then compared to the provided label.\n",
    "\n",
    "For our case let's use the following `strip_response` function to trim white space off the responses before comparing them to their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57fad6f9-cee9-47d1-b1c8-0477c930c5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_response(response: str) -> str:\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d46d940-5035-4b81-a99e-2d014962543c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c28e8bb6dc4402aa0b4fd51df093d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 correct\n",
      "Accuracy: 1.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.00'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt43b.evaluate(prompts_and_answers[:3], get_clean_prediction=strip_response, tokens_to_generate=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f81136-cc70-403e-b69b-bd78b63549bb",
   "metadata": {},
   "source": [
    "Having stripped off the white space we see the GPT43B model performing as well as we observed above.\n",
    "\n",
    "It's worth mentioning that the intended use of `evaluate` is to evaluate model performance across datasets and is not a replacement for `generate`. If you need to view individual model responses for any reason, switch to using `generate` on the data samples in question so you can see model output directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44cf045-b2c4-43f3-ae6f-b6e96eb1cf31",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dfde1d-98c8-47e5-a03c-ea390a7b9479",
   "metadata": {},
   "source": [
    "## Try Zero-shot Prompting With Full Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb51cf0-7df5-4e0e-ba5a-df1ebad3cbbe",
   "metadata": {},
   "source": [
    "Now we will iterate through all 3 GPT models and evaluate their performance on our test set. In doing so we will also introduce another feature of the `generate` method which is to write evaluation results to a csv file. Later we will visualize the results to better observe how each of the models perform in different prompting and fine-tuning scenarios.\n",
    "\n",
    "We will be writing to `'experiment_results/pubmed_experiment_results.csv'`. This file will contain results for all of our experiments with the PubMedQA data including zero, one and few-shot prompting, p-tuning and LoRA, with a variety of models.\n",
    "\n",
    "The model instance itself will provide the model name to the experiment results file, so we don't need to pass that in manually. We supply `experiment_name` to specify the customization conditions for this particular run, in our case, \"Zero Shot\".\n",
    "\n",
    "The following, which runs the whole `prompts_and_answers` dataset through 3 GPT models will take a minute or two to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9df124ca-b984-4c04-b860-198bb4f94d09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8afc2b669d546308b4c4005fcf4d0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT8B\n",
      "-----\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2c851bee8664a29a367929adb61cb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/150 correct\n",
      "Accuracy: 0.00\n",
      "\n",
      "GPT20B\n",
      "------\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "931f24086ae74b8ba8994d19e9f6f3fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ServerSideError",
     "evalue": "Request failed with HTTP Status Code 504 GATEWAY TIMEOUT **Solution**: Server is unable to handle your request right now. Please retry your request after a brief wait. If this problem persist, please contact NeMo LLM team **Full response**: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center></center>\r\n</body>\r\n</html>\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerSideError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m underline \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlen\u001b[39m(name)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00munderline\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts_and_answers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_clean_prediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrip_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mwrite_results_to_csv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mZero Shot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcsv_file_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexperiment_results/pubmed_experiment_results.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokens_to_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/workspace/dli/2-PubMedQA/llm_utils/nemo_service_models.py:135\u001b[0m, in \u001b[0;36mNemoServiceBaseModel.evaluate\u001b[0;34m(self, prompts_with_labels, get_clean_prediction, print_results, experiment_name, model_description, write_results_to_csv, csv_file_name, **kwargs)\u001b[0m\n\u001b[1;32m    132\u001b[0m num_correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt, label \u001b[38;5;129;01min\u001b[39;00m tqdm(prompts_with_labels):\n\u001b[0;32m--> 135\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     clean_prediction \u001b[38;5;241m=\u001b[39m get_clean_prediction(prediction) \u001b[38;5;28;01mif\u001b[39;00m get_clean_prediction \u001b[38;5;28;01melse\u001b[39;00m prediction\n\u001b[1;32m    138\u001b[0m     labels\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "File \u001b[0;32m/workspace/dli/2-PubMedQA/llm_utils/nemo_service_models.py:67\u001b[0m, in \u001b[0;36mNemoServiceBaseModel.generate\u001b[0;34m(self, prompt, return_type, tokens_to_generate, temperature, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_stream(prompt, model, customization_id, tokens_to_generate, temperature, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustomization_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens_to_generate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/dli/2-PubMedQA/llm_utils/nemo_service_models.py:97\u001b[0m, in \u001b[0;36mNemoServiceBaseModel._generate_text\u001b[0;34m(self, prompt, model, customization_id, tokens_to_generate, temperature, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03mInternal method to generate text in text mode.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     95\u001b[0m conn \u001b[38;5;241m=\u001b[39m NemoLLM(api_host\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_host, api_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key)\n\u001b[0;32m---> 97\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustomization_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustomization_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens_to_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokens_to_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/nemollm/api.py:460\u001b[0m, in \u001b[0;36mNemoLLM.generate\u001b[0;34m(self, model, prompt, customization_id, return_type, tokens_to_generate, logprobs, temperature, top_p, top_k, stop, random_seed, repetition_penalty, beam_search_diversity_rate, beam_width, length_penalty, disable_logging)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 460\u001b[0m     \u001b[43mNemoLLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NemoLLM\u001b[38;5;241m.\u001b[39mpost_process_generate_response(response, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/nemollm/api.py:130\u001b[0m, in \u001b[0;36mNemoLLM.handle_response\u001b[0;34m(response, stream)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ClientSideError(status_code\u001b[38;5;241m=\u001b[39mstatus_code, reason\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mreason, decoded_content\u001b[38;5;241m=\u001b[39mdecoded_content)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# server side errors\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ServerSideError(status_code\u001b[38;5;241m=\u001b[39mstatus_code, reason\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mreason, decoded_content\u001b[38;5;241m=\u001b[39mdecoded_content)\n",
      "\u001b[0;31mServerSideError\u001b[0m: Request failed with HTTP Status Code 504 GATEWAY TIMEOUT **Solution**: Server is unable to handle your request right now. Please retry your request after a brief wait. If this problem persist, please contact NeMo LLM team **Full response**: <html>\r\n<head><title>504 Gateway Time-out</title></head>\r\n<body>\r\n<center><h1>504 Gateway Time-out</h1></center>\r\n<hr><center></center>\r\n</body>\r\n</html>\r\n"
     ]
    }
   ],
   "source": [
    "for name, llm in tqdm(llms.items()):\n",
    "    underline = \"-\"*len(name)\n",
    "    print(f'{name.upper()}\\n{underline}\\n')\n",
    "    \n",
    "    llm.evaluate(prompts_and_answers,\n",
    "                get_clean_prediction=strip_response,\n",
    "                write_results_to_csv=True,\n",
    "                experiment_name='Zero Shot',\n",
    "                csv_file_name='experiment_results/pubmed_experiment_results.csv',\n",
    "                tokens_to_generate=1)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb71fd00-a326-404d-a2a4-ae6fea0e18d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142dd301-cd83-4b7f-b4f7-e7c98e5a052e",
   "metadata": {},
   "source": [
    "## View Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac76d1-f99f-4508-816d-05686d683b75",
   "metadata": {},
   "source": [
    "In addition to the printouts above, each model instance has an `experiment_results` property we can inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce68d17-e657-4e72-b6f0-54dd51f205fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for llm in llms.values():\n",
    "    print(llm.experiment_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d538e348-8200-4742-b745-52a03b8e1715",
   "metadata": {},
   "source": [
    "Additionally, we have provided a `plot_experiment_results` helper for better visualization, especially when we start to conduct multiple experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d3af3-ae5a-4a02-9e24-53c07c7d6a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_experiment_results('experiment_results/pubmed_experiment_results.csv')\n",
    "\n",
    "# We also provide solution/reference results in case they are needed.\n",
    "# plot_experiment_results('experiment_results/solutions/zero_shot_pubmed_experiment_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5b2cb0-04ed-4517-a62d-6a481a410035",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20b082-84c0-43d2-9b12-7624501fca3c",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a562ac87-c8f1-4682-a4d1-e75306ec4823",
   "metadata": {},
   "source": [
    "Much like we observed above with the small sample size, the model's have performed to varying degrees, largely relative to their size. 43B appears to be doing quite well, 20B didn't fail entirely, but only got some responses correct, and 8B didn't get a single response correct."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
